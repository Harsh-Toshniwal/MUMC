# MUMC
## [Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering](https://arxiv.org/abs/2307.05314)
This is the official implementation of `MUMC` for the medical visual question answering, which was accepted by [MICCAI-2023](https://conferences.miccai.org/2023/en/default.asp).
Our proposal achieves superior accuracy in comparison with other state-of-the-art (sota) methods on three public medical VQA datasets: [VQA-RAD dataset](https://www.nature.com/articles/sdata2018251#data-citations), [PathVQA dataset](https://arxiv.org/abs/2003.10286) and [Slake dataset](https://arxiv.org/abs/2102.09542). Paper link [here](https://arxiv.org/abs/2211.13594).

This repository is based on and inspired by our previous [work](https://github.com/pengfeiliHEU/M2I2) and @Junnan Li's [work](https://github.com/salesforce/ALBEF). We sincerely thank for their sharing of the codes.


